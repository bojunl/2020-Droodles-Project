{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Droodles_RepetitiveTextGeneration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDiQBz5bKY8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
        "\n",
        "\"\"\"\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\"\"\"\n",
        "\n",
        "# Stanford POS tagger\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "stanford_dir = \" \"\n",
        "modelfile = \" \"\n",
        "jarfile = \" \"\n",
        "S_tagger = StanfordPOSTagger(model_filename=modelfile, path_to_jar=jarfile)\n",
        "\n",
        "# Load Google News Word Embeddings\n",
        "embed_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) \n",
        "\n",
        "SEQUENCE_LEN = 10\n",
        "DIVERSITY = 0.7\n",
        "QUANTITY = 15\n",
        "TAGGER = 'NLTK'\n",
        "NUM_GENERATE_TEXT = 100\n",
        "corpusName = 'jokes.txt'\n",
        "model_file = \"LSTM_JOKE-epoch037-val_acc0.2754\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJoL4VHrP3fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up\n",
        "\n",
        "with io.open(corpusName, encoding='utf-8') as f:\n",
        "    text = f.read().lower().replace('\\n', ' \\n ')\n",
        "\n",
        "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "\n",
        "for i in range(len(text_in_words)):\n",
        "  text_in_words[i] = re.sub(r'[^\\w\\s\\'\\-\\/]','',text_in_words[i])\n",
        "\n",
        "word_freq = {}\n",
        "for word in text_in_words:\n",
        "    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "ignored_words = set()\n",
        "for k, v in word_freq.items():\n",
        "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
        "        ignored_words.add(k)\n",
        "\n",
        "words = set(text_in_words)\n",
        "words = sorted(set(words) - ignored_words)\n",
        "\n",
        "word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "indices_word = dict((i, c) for i, c in enumerate(words))\n",
        "\n",
        "STEP = 1\n",
        "sentences = []\n",
        "next_words = []\n",
        "ignored = 0\n",
        "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
        "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
        "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
        "    else:\n",
        "        ignored = ignored+1\n",
        "\n",
        "def shuffle_and_split_training_set(sentences_original, labels_original, percentage_test=10):\n",
        "    tmp_sentences = []\n",
        "    tmp_next_char = []\n",
        "    for i in np.random.permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_char.append(labels_original[i])\n",
        "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_char[:cut_index], tmp_next_char[cut_index:]\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "sentences, next_words, sentences_test, next_words_test = shuffle_and_split_training_set(sentences, next_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsbdEhvTRs9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the trained model\n",
        "\n",
        "model = load_model(model_file)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzokY_7cK1ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tag each word in the generated sentences\n",
        "\n",
        "def postag(taggerchoice, text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  if taggerchoice == 'NLTK':\n",
        "    return nltk.pos_tag(tokens)\n",
        "  elif taggerchoice == 'Stanford':\n",
        "    return S_tagger.tag(tokens)\n",
        "  else:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQyMa4IgR28f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate sentences with the random seed\n",
        "\n",
        "def generate_text(model, indices_word, word_indices, seed, sequence_length, diversity, quantity):\n",
        "    sentence = seed.split(\" \")\n",
        "    generated_text = \"\"\n",
        "    for i in range(quantity):\n",
        "        x_pred = np.zeros((1, sequence_length, len(words)))\n",
        "        for t, word in enumerate(sentence):\n",
        "            x_pred[0, t, word_indices[word]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, diversity)\n",
        "        next_word = indices_word[next_index]\n",
        "\n",
        "        sentence = sentence[1:]\n",
        "        sentence.append(next_word)\n",
        "\n",
        "        generated_text = generated_text + \" \" + next_word\n",
        "    return generated_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri2m6_PoVSS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test if the sentence has exactly 2 nouns\n",
        "\n",
        "def test_noun(tagged_text):\n",
        "  num_noun = 0\n",
        "  for word, tag in tagged_text:\n",
        "    if tag == 'NN':\n",
        "      num_noun += 1\n",
        "  if num_noun == 2:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqBw7-MiOiad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Sentences\n",
        "\n",
        "generated = 0\n",
        "sentence_list = []\n",
        "\n",
        "while generated < NUM_GENERATE_TEXT:\n",
        "  seed_index = np.random.randint(len(sentences+sentences_test))\n",
        "  seed = (sentences+sentences_test)[seed_index]\n",
        "  new_sentence = seed + generate_text(model, indices_word, word_indices, seed, SEQUENCE_LEN, DIVERSITY, QUANTITY)\n",
        "  tagged_sentence = postag(TAGGER, new_sentence)\n",
        "  if test_noun(tagged_sentence) == True:\n",
        "    generated += 1\n",
        "    sentence_list.append(tagged_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epefA0YpX8_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Retrieve identified objects\n",
        "\n",
        "file = open(\"identified.txt\", \"r\")\n",
        "objects = file.readlines()\n",
        "objects = [obj.strip() for obj in objects]\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6t-G5BeX0rV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace nouns in the generated sentences with identified objects and calculate sums of cosine similarities\n",
        "\n",
        "def ReplaceNoun(objects, tag_sentences):\n",
        "  replaced = []\n",
        "  sum_similarity = []\n",
        "  for sent in tag_sentences:\n",
        "    noun_idx = []\n",
        "    for i in range(len(sent)):\n",
        "      if sent[i][1] == 'NN':\n",
        "        noun_idx.append(i)\n",
        "    relative_a = float(embed_model.similarity(sent[noun_idx[0]][0], objects[0])) / float(embed_model.similarity(sent[noun_idx[0]][0], objects[1]))\n",
        "    relative_b = float(embed_model.similarity(sent[noun_idx[1]][0], objects[0])) / float(embed_model.similarity(sent[noun_idx[1]][0], objects[1]))\n",
        "    if relative_a > relative_b:\n",
        "      temp_idx1 = 1\n",
        "      temp_idx2 = 0\n",
        "      sum_similarity.append(float(embed_model.similarity(sent[noun_idx[0]][0], objects[0])) + float(embed_model.similarity(sent[noun_idx[1]][0], objects[1])))\n",
        "    else:\n",
        "      temp_idx1 = 0\n",
        "      temp_idx2 = 1\n",
        "      sum_similarity.append(float(embed_model.similarity(sent[noun_idx[0]][0], objects[1])) + float(embed_model.similarity(sent[noun_idx[1]][0], objects[0])))\n",
        "    temp_sentence = \"\"\n",
        "    for j in range(len(sent)):\n",
        "      if j == noun_idx[temp_idx2]:\n",
        "        temp_sentence = temp_sentence + \" \" + objects[0]\n",
        "      elif j == noun_idx[temp_idx1]:\n",
        "        temp_sentence = temp_sentence + \" \" + objects[1]\n",
        "      else:\n",
        "        temp_sentence = temp_sentence + \" \" + sent[j][0]\n",
        "    replaced.append(temp_sentence)\n",
        "  return replaced, sum_similarity\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8NqkUXYvjXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "candidate_sentence, sim_score = ReplaceNoun(objects, sentence_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqsO7Iafvl89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the optimal sentence\n",
        "\n",
        "watermark = 0.0\n",
        "choice = 0\n",
        "\n",
        "for idx in range(NUM_GENERATE_TEXT):\n",
        "  if sim_score[idx] > watermark:\n",
        "    watermark = sim_score[idx]\n",
        "    choice = idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLqELAwQ29-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = open(\"output.txt\", \"w\")\n",
        "file1.write(candidate_sentence[choice][1:])\n",
        "file1.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}